{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## install and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (4.21.1)\n",
      "Requirement already satisfied: filelock in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (0.8.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from transformers) (2022.7.25)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests->transformers) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (0.1.97)\n",
      "Requirement already satisfied: wandb in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (0.13.1)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: pathtools in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (47.1.0)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (1.9.4)\n",
      "Requirement already satisfied: setproctitle in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: PyYAML in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: six>=1.13.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (1.16.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: importlib-metadata in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.3.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.1)\n",
      "Requirement already satisfied: torch in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/hye/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages (from torch) (4.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "!pip install wandb\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import wandb\n",
    "from easydict import EasyDict as edict\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    #GPT2Tokenizer as BaseGPT2Tokenizer,\n",
    "    PreTrainedTokenizer,\n",
    "    PreTrainedTokenizerFast,\n",
    "    BertTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Trainer,\n",
    "\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers.models.encoder_decoder.modeling_encoder_decoder import EncoderDecoderModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /content/drive/MyDrive/GoormProject/GoormProject3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMT_enko_999\n"
     ]
    }
   ],
   "source": [
    "args = edict({'do_wandb' : False,\n",
    "              'w_project': 'NMT_enko',\n",
    "              'w_entity': 'goorm-project-nlp-team-1', # WandB ID\n",
    "              'learning_rate': 2e-4,\n",
    "              'batch_size': 16,\n",
    "              'accumulate': 16,\n",
    "              'epochs': 10,\n",
    "              'seed': 42,\n",
    "              'src_pt' : 'bert-base-cased',\n",
    "              'trg_pt': 'skt/kogpt2-base-v2', \n",
    "              'max_length': 512,\n",
    "              'earlystopping' : True,\n",
    "              'warmup_proportion' : 0.1,\n",
    "              'patience' : 0.5,\n",
    "              })\n",
    "args['NAME'] = ''f'{args.w_project}_{random.randrange(0, 1024)}'\n",
    "print(args.NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "seed_everything(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairedDataset:\n",
    "    def __init__(self, \n",
    "        src_tokenizer: PreTrainedTokenizer, tgt_tokenizer: PreTrainedTokenizer,\n",
    "        file_path: str\n",
    "    ):\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.trg_tokenizer = tgt_tokenizer\n",
    "        with open(file_path, 'r') as fd:\n",
    "            self.data = [row[1:] for row in csv.reader(fd)][1:]\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        src, trg = self.data[index]\n",
    "        embeddings = self.src_tokenizer(src, return_attention_mask=False, return_token_type_ids=False)\n",
    "        embeddings['labels'] = self.trg_tokenizer(trg, return_attention_mask=False)['input_ids']\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer, dataset, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoGPT2Tokenizer(PreTrainedTokenizerFast):\n",
    "    def build_inputs_with_special_tokens(self, token_ids: List[int], _) -> List[int]:\n",
    "        return token_ids + [self.eos_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'KoGPT2Tokenizer'.\n"
     ]
    }
   ],
   "source": [
    "src_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "trg_tokenizer = KoGPT2Tokenizer.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PairedDataset(src_tokenizer, trg_tokenizer, 'data/기술과학_train_en-ko.csv')\n",
    "eval_dataset = PairedDataset(src_tokenizer, trg_tokenizer, 'data/기술과학_valid_en-ko.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['transformer.h.11.crossattention.q_attn.weight', 'transformer.h.10.crossattention.c_proj.weight', 'transformer.h.8.crossattention.masked_bias', 'transformer.h.2.crossattention.bias', 'transformer.h.11.crossattention.c_attn.weight', 'transformer.h.10.crossattention.bias', 'transformer.h.8.crossattention.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.6.crossattention.masked_bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.1.crossattention.masked_bias', 'transformer.h.9.crossattention.masked_bias', 'transformer.h.0.crossattention.bias', 'transformer.h.7.crossattention.c_proj.weight', 'transformer.h.8.crossattention.q_attn.weight', 'transformer.h.6.crossattention.bias', 'transformer.h.11.crossattention.bias', 'transformer.h.6.ln_cross_attn.weight', 'transformer.h.1.crossattention.bias', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.4.crossattention.masked_bias', 'transformer.h.9.crossattention.c_attn.weight', 'transformer.h.5.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.6.crossattention.q_attn.weight', 'transformer.h.9.crossattention.c_proj.weight', 'transformer.h.7.crossattention.c_attn.weight', 'transformer.h.11.crossattention.c_proj.weight', 'transformer.h.11.ln_cross_attn.weight', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.8.crossattention.c_proj.bias', 'transformer.h.5.crossattention.bias', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.7.crossattention.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.8.ln_cross_attn.weight', 'transformer.h.5.crossattention.masked_bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.9.crossattention.bias', 'transformer.h.10.ln_cross_attn.weight', 'transformer.h.9.ln_cross_attn.weight', 'transformer.h.4.crossattention.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.6.crossattention.c_proj.weight', 'transformer.h.3.crossattention.masked_bias', 'transformer.h.6.crossattention.c_attn.weight', 'transformer.h.7.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.7.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.8.crossattention.c_attn.weight', 'transformer.h.7.crossattention.c_proj.bias', 'transformer.h.9.crossattention.c_proj.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.9.crossattention.q_attn.weight', 'transformer.h.10.crossattention.c_attn.weight', 'transformer.h.7.crossattention.masked_bias', 'transformer.h.10.crossattention.q_attn.weight', 'transformer.h.10.crossattention.c_proj.bias', 'transformer.h.11.crossattention.c_proj.bias', 'transformer.h.0.crossattention.masked_bias', 'transformer.h.6.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.8.crossattention.c_proj.weight', 'transformer.h.2.crossattention.masked_bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.11.crossattention.masked_bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.3.crossattention.bias', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.10.crossattention.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    'bert-base-cased',\n",
    "    'skt/kogpt2-base-v2',\n",
    "    pad_token_id=trg_tokenizer.bos_token_id\n",
    ")\n",
    "model.config.decoder_start_token_id = trg_tokenizer.bos_token_id\n",
    "model.config.early_stopping = True\n",
    "#model.config.max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForSeq2Seq(src_tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = Seq2SeqTrainingArguments(\n",
    "    output_dir='dump',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=30,\n",
    "    # per_device_train_batch_size=64,\n",
    "    # per_device_eval_batch_size=64,\n",
    "    auto_find_batch_size = True,\n",
    "    warmup_ratio=0.1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    save_total_limit=5,\n",
    "    dataloader_num_workers=1,\n",
    "    fp16=False, # True only CUDA\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    arguments,\n",
    "    data_collator=collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=eval_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.do_wandb :\n",
    "  wandb.login()\n",
    "  run = wandb.init(project = args.w_project, entity = args.w_entity)\n",
    "  wandb.run.name = args.NAME\n",
    "  wandb.config.learning_rate = args.learning_rate\n",
    "  wandb.config.epochs = args.epochs\n",
    "  wandb.config.batch_size = args.batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cuda setting and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda memory error 피하기\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nfind_executable_batch_size requires the accelerate library but it was not found in your environment. You can install it with pip:\n`pip install accelerate`\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ry/h4bnhk0x4xj4981zz6n6xgtc0000gn/T/ipykernel_39707/4285853078.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"model/{args.NAME}_best_model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1495\u001b[0m         inner_training_loop = find_executable_batch_size(\n\u001b[0;32m-> 1496\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m         )\n\u001b[1;32m   1498\u001b[0m         return inner_training_loop(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36mfind_executable_batch_size\u001b[0;34m(function, starting_batch_size, auto_find_batch_size)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_executable_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"accelerate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmem_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.13/envs/goorm/lib/python3.7/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[0mfailed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchecks\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfailed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nfind_executable_batch_size requires the accelerate library but it was not found in your environment. You can install it with pip:\n`pip install accelerate`\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(f\"model/{args.NAME}_best_model\")\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(f\"model/{args.NAME}_best_model\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "model.config.decoder_start_token_id = trg_tokenizer.bos_token_id\n",
    "model.config.early_stopping = True\n",
    "model.config.max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ry/h4bnhk0x4xj4981zz6n6xgtc0000gn/T/ipykernel_39707/3645136972.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtestset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/test_investing.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtestset_mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'내용'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "testset = pd.read_csv('data/test_investing.csv')\n",
    "\n",
    "testset_mt = []\n",
    "for i in range(len(testset)) :\n",
    "    text = testset['내용'][i]\n",
    "    embeddings = src_tokenizer(text, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt')\n",
    "    embeddings = {k: v.cuda() for k, v in embeddings.items()}\n",
    "    output = model.generate(**embeddings)[0, 1:-1]\n",
    "    text_mt = trg_tokenizer.decode(output.cpu())\n",
    "    testset_mt.append(text_mt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "trg_tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "model = EncoderDecoderModel.from_pretrained(f\"model/{args.NAME}_best_model\")\n",
    "model.eval()\n",
    "model.cuda()\n",
    "model.config.decoder_start_token_id = trg_tokenizer.bos_token_id\n",
    "model.config.early_stopping = True\n",
    "model.config.max_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "eng = widgets.Textarea(\n",
    "    placeholder='번역할 영어',\n",
    "    description=\"입력\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(\n",
    "    description='번역!',\n",
    "    disabled=False,\n",
    "    tooltip='해당 기사를 번역합니다.'\n",
    ")\n",
    "\n",
    "kor = widgets.Textarea(\n",
    "    description=\"출력\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "def translate(_):\n",
    "    eng.value = \"\"\n",
    "    text = kor.get_interact_value()\n",
    "    embeddings = src_tokenizer(text, return_attention_mask=False, return_token_type_ids=False, return_tensors='pt')\n",
    "    embeddings = {k: v.cuda() for k, v in embeddings.items()}\n",
    "    output = model.generate(**embeddings)[0, 1:-1]\n",
    "    eng.value = trg_tokenizer.decode(output.cpu())\n",
    "\n",
    "button.on_click(translate)\n",
    "display(eng, button, kor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('goorm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7c9af6041159dfe6c99c828ac23c7a151cdd31a3e0c79836f752ede52d58adc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
